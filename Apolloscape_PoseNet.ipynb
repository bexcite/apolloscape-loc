{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoseNet for Self-Localization Task on Apolloscape Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PoseNet implementation on Apolloscape dataset. It's a part of the localization methods exploration for Apolloscape Self-Localization Challenge on [ECCV 2018](http://apolloscape.auto/ECCV/challenge.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all dependency\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "\n",
    "from datasets.apolloscape import Apolloscape\n",
    "\n",
    "from utils.common import draw_poses\n",
    "from utils.common import draw_record\n",
    "from utils.common import imshow\n",
    "from utils.common import save_checkpoint\n",
    "from utils.common import AverageMeter\n",
    "from utils.common import calc_poses_params, quaternion_angular_error\n",
    "\n",
    "from models.posenet import PoseNet, PoseNetCriterion\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Apolloscape Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`APOLLO_PATH` is a location with unpacked Apolloscape datasets, e.g. `$APOLLO_PATH/road02_seg` or `$APOLLO_PATH/zpark`. Download data from [Apolloscape page](http://apolloscape.auto/scene.html) and unpack it to the `APOLLO_PATH` dir. Let's we've symbolically linked `APOLLO_PATH` folder to `./data/apolloscape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APOLLO_PATH = \"./data/apolloscape\"\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Resize data before using\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(260),\n",
    "    transforms.CenterCrop(250),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "experiment_name = 'zpark_L4'\n",
    "\n",
    "stereo = False\n",
    "\n",
    "shuffle = True\n",
    "\n",
    "batch_size = 80\n",
    "\n",
    "train_record = 'Record001'\n",
    "train_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"zpark-sample\",\n",
    "                             transform=transform, record=train_record, normalize_poses=True,\n",
    "                             pose_format='quat', train=True, cache_transform=True, stereo=stereo)\n",
    "\n",
    "val_record = 'Record013'\n",
    "val_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"zpark-sample\",\n",
    "                             transform=transform, record=val_record, normalize_poses=True,\n",
    "                             pose_format='quat', train=False, cache_transform=True, stereo=stereo)\n",
    "\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle) # batch_size = 75\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle) # batch_size = 75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Train and Val datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize poses from `train` and `val` datasets to check that it covers all val path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw datasets\n",
    "draw_record(train_dataset)\n",
    "plt.show()\n",
    "\n",
    "draw_record(val_dataset)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the primary device and use it for training/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('device = {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train function runs one epoch through training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(train_loader, model, criterion, optimizer, epoch, max_epoch, log_freq=1, print_sum=True,\n",
    "          poses_mean=None, poses_std=None, stereo = True):\n",
    "    \n",
    "    # switch model to training\n",
    "    model.train()\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    epoch_time = time.time()\n",
    "    \n",
    "    gt_poses = np.empty((0, 7))\n",
    "    pred_poses = np.empty((0, 7))\n",
    "\n",
    "    \n",
    "    end = time.time()\n",
    "    for idx, (batch_images, batch_poses) in enumerate(train_loader):\n",
    "        data_time = (time.time() - end)\n",
    "        \n",
    "        if stereo:\n",
    "            batch_images = [x.to(device) for x in batch_images]\n",
    "            batch_poses = [x.to(device) for x in batch_poses]\n",
    "        else:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_poses = batch_poses.to(device)\n",
    "        \n",
    "        out = model(batch_images)\n",
    "        loss = criterion(out, batch_poses)\n",
    "#         print('loss = {}'.format(loss))\n",
    "\n",
    "        \n",
    "        losses.update(loss, len(batch_images) * batch_images[0].size(0) if stereo\n",
    "                else batch_images.size(0))\n",
    "        \n",
    "        # Training step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # move data to cpu & numpy\n",
    "        if stereo:\n",
    "            bp = [x.detach().cpu().numpy() for x in batch_poses]\n",
    "            outp = [x.detach().cpu().numpy() for x in out]\n",
    "            gt_poses = np.vstack((gt_poses, *bp))\n",
    "            pred_poses = np.vstack((pred_poses, *outp))\n",
    "        else:\n",
    "            bp = batch_poses.detach().cpu().numpy()\n",
    "            outp = out.detach().cpu().numpy()\n",
    "            gt_poses = np.vstack((gt_poses, bp))\n",
    "            pred_poses = np.vstack((pred_poses, outp))\n",
    "        \n",
    "        \n",
    "        batch_time = (time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if log_freq != 0 and idx % log_freq == 0:\n",
    "            print('Epoch: [{}/{}]\\tBatch: [{}/{}]\\t'\n",
    "                  'Time: {batch_time:.3f}\\t'\n",
    "                  'Data Time: {data_time:.3f}\\t'\n",
    "                  'Loss: {losses.val:.3f}\\t'\n",
    "                  'Avg Loss: {losses.avg:.3f}\\t'.format(\n",
    "                   epoch, max_epoch - 1, idx, len(train_loader) - 1,\n",
    "                   batch_time=batch_time, data_time=data_time, losses=losses))\n",
    "\n",
    "            \n",
    "    # un-normalize translation\n",
    "    unnorm = (poses_mean is not None) and (poses_std is not None)\n",
    "    if unnorm:\n",
    "        gt_poses[:, :3] = gt_poses[:, :3] * poses_std + poses_mean\n",
    "        pred_poses[:, :3] = pred_poses[:, :3] * poses_std + poses_mean\n",
    "    \n",
    "    t_loss = np.asarray([np.linalg.norm(p - t) for p, t in zip(pred_poses[:, :3], gt_poses[:, :3])])\n",
    "    q_loss = np.asarray([quaternion_angular_error(p, t) for p, t in zip(pred_poses[:, 3:], gt_poses[:, 3:])])\n",
    "\n",
    "#     if unnorm:\n",
    "#         print('poses_std = {:.3f}'.format(np.linalg.norm(poses_std)))\n",
    "#     print('T: median = {:.3f}, mean = {:.3f}'.format(np.median(t_loss), np.mean(t_loss)))\n",
    "#     print('R: median = {:.3f}, mean = {:.3f}'.format(np.median(q_loss), np.mean(q_loss)))\n",
    "\n",
    "            \n",
    "    if print_sum:\n",
    "        print('Ep: [{}/{}]\\tTrain Loss: {:.3f}\\tTe: {:.3f}\\tRe: {:.3f}\\t Et: {:.2f}s'.format(\n",
    "            epoch, max_epoch - 1, losses.avg, np.mean(t_loss), np.mean(q_loss),\n",
    "            (time.time() - epoch_time)))\n",
    "        \n",
    "#     return losses.avg\n",
    "    \n",
    "    \n",
    "def validate(val_loader, model, criterion, epoch, log_freq=1, print_sum=True, stereo=True):\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    # set model to evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        epoch_time = time.time()\n",
    "        end = time.time()\n",
    "        for idx, (batch_images, batch_poses) in enumerate(val_loader):\n",
    "            data_time = time.time() - end\n",
    "            \n",
    "            if stereo:\n",
    "                batch_images = [x.to(device) for x in batch_images]\n",
    "                batch_poses = [x.to(device) for x in batch_poses]\n",
    "            else:\n",
    "                batch_images = batch_images.to(device)\n",
    "                batch_poses = batch_poses.to(device)\n",
    "            \n",
    "            # compute model output\n",
    "            out = model(batch_images)\n",
    "            loss = criterion(out, batch_poses)\n",
    "            \n",
    "            losses.update(loss, len(batch_images) * batch_images[0].size(0) if stereo\n",
    "                    else batch_images.size(0))\n",
    "            \n",
    "            batch_time = time.time() - end\n",
    "            end = time.time()\n",
    "            \n",
    "            if log_freq != 0 and idx % log_freq == 0:\n",
    "                print('Val Epoch: {}\\t'\n",
    "                      'Time: {batch_time:.3f}\\t'\n",
    "                      'Data Time: {data_time:.3f}\\t'\n",
    "                      'Loss: {losses.val:.3f}\\t'\n",
    "                      'Avg Loss: {losses.avg:.3f}'.format(\n",
    "                       epoch, batch_time=batch_time, data_time=data_time, losses=losses))\n",
    "                \n",
    "    \n",
    "    if print_sum:\n",
    "        print('Epoch: [{}]\\tValidation Loss: {:.3f}\\tEpoch time: {:.3f}'.format(epoch, losses.avg,\n",
    "                                                                               (time.time() - epoch_time)))\n",
    "        \n",
    "#     return losses.avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Criterion and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PoseNet implementation is a ResNet based feature extractor that ends with custom fully-connected regressor layers for translation (3D pose) and rotation (quaternion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pretrained feature extractor\n",
    "# feature_extractor = models.resnet18(pretrained=True)\n",
    "feature_extractor = models.resnet34(pretrained=False)\n",
    "# feature_extractor = models.resnet50(pretrained=True)\n",
    "\n",
    "# Num features for the last layer before pose regressor\n",
    "num_features = 2048\n",
    "\n",
    "# Create model\n",
    "model = PoseNet(feature_extractor, num_features=num_features)\n",
    "model = model.to(device)\n",
    "\n",
    "# Criterion\n",
    "criterion = PoseNetCriterion(stereo=stereo, beta=500.0)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=0.0005)\n",
    "\n",
    "start_epoch = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore from Chekpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps to continue training later or just use it for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore from checkpoint\n",
    "checkpoint_file = '_checkpoints/20180812_143625_nb_zpark_L4_e600.pth.tar'\n",
    "\n",
    "if 'checkpoint_file' in locals() and checkpoint_file is not None:\n",
    "    if os.path.isfile(checkpoint_file):\n",
    "        print('Loading from checkpoint: {}'.format(checkpoint_file))\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optim_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop that runs for `n_epochs` with validation every `val_freq` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'start_epoch' not in locals():\n",
    "    start_epoch = 0\n",
    "    \n",
    "n_epochs = start_epoch + 500\n",
    "\n",
    "print('Training ...')\n",
    "val_freq = 5\n",
    "for e in range(start_epoch, n_epochs):\n",
    "    train(train_dataloader, model, criterion, optimizer, e, n_epochs, log_freq=0,\n",
    "         poses_mean=train_dataset.poses_mean, poses_std=train_dataset.poses_std, stereo=stereo)\n",
    "    if e % val_freq == 0:\n",
    "        end = time.time()\n",
    "        validate(val_dataloader, model, criterion, e, log_freq=0, stereo=stereo)\n",
    "\n",
    "start_epoch = n_epochs\n",
    "\n",
    "print('n_epochs = {}'.format(n_epochs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Error on Validation and Train Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate translation and rotation error of the predicted poses on train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_results_pred_gt(model, dataloader, poses_mean, poses_std, stereo=True):\n",
    "    model.eval()\n",
    "\n",
    "    gt_poses = np.empty((0, 7))\n",
    "    pred_poses = np.empty((0, 7))\n",
    "\n",
    "    for idx, (batch_images, batch_poses) in enumerate(dataloader):\n",
    "        \n",
    "        if stereo:\n",
    "            batch_images = [x.to(device) for x in batch_images]\n",
    "            batch_poses = [x.to(device) for x in batch_poses]\n",
    "        else:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_poses = batch_poses.to(device)\n",
    "\n",
    "\n",
    "        out = model(batch_images)\n",
    "        \n",
    "        loss = criterion(out, batch_poses)\n",
    "#         print('loss = {}'.format(loss))\n",
    "\n",
    "        # move data to cpu & numpy\n",
    "        if stereo:\n",
    "            batch_poses = [x.detach().cpu().numpy() for x in batch_poses]\n",
    "            out = [x.detach().cpu().numpy() for x in out]\n",
    "            gt_poses = np.vstack((gt_poses, *batch_poses))\n",
    "            pred_poses = np.vstack((pred_poses, *out))\n",
    "        else:\n",
    "            bp = batch_poses.detach().cpu().numpy()\n",
    "            outp = out.detach().cpu().numpy()\n",
    "            gt_poses = np.vstack((gt_poses, bp))\n",
    "            pred_poses = np.vstack((pred_poses, outp))\n",
    "\n",
    "\n",
    "        \n",
    "    # un-normalize translation\n",
    "    gt_poses[:, :3] = gt_poses[:, :3] * poses_std + poses_mean\n",
    "    pred_poses[:, :3] = pred_poses[:, :3] * poses_std + poses_mean\n",
    "    \n",
    "    return pred_poses, gt_poses\n",
    "\n",
    "\n",
    "# Get mean and std from dataset\n",
    "poses_mean = val_dataset.poses_mean\n",
    "poses_std = val_dataset.poses_std\n",
    "\n",
    "\n",
    "\n",
    "print('\\n=== Test Training Dataset ======')\n",
    "pred_poses, gt_poses = model_results_pred_gt(model, train_dataloader, poses_mean, poses_std, stereo=stereo)\n",
    "\n",
    "print('gt_poses = {}'.format(gt_poses.shape))\n",
    "print('pred_poses = {}'.format(pred_poses.shape))\n",
    "t_loss = np.asarray([np.linalg.norm(p - t) for p, t in zip(pred_poses[:, :3], gt_poses[:, :3])])\n",
    "q_loss = np.asarray([quaternion_angular_error(p, t) for p, t in zip(pred_poses[:, 3:], gt_poses[:, 3:])])\n",
    "\n",
    "print('poses_std = {:.3f}'.format(np.linalg.norm(poses_std)))\n",
    "print('Translation(T) error in meters and Rotation(R) error in degrees:')\n",
    "print('T: median = {:.3f}, mean = {:.3f}'.format(np.median(t_loss), np.mean(t_loss)))\n",
    "print('R: median = {:.3f}, mean = {:.3f}'.format(np.median(q_loss), np.mean(q_loss)))\n",
    "\n",
    "# Save for later visualization\n",
    "pred_poses_train = pred_poses\n",
    "gt_poses_train = gt_poses\n",
    "\n",
    "\n",
    "print('\\n=== Test Validation Dataset ======')\n",
    "pred_poses, gt_poses = model_results_pred_gt(model, val_dataloader, poses_mean, poses_std, stereo=stereo)\n",
    "\n",
    "print('gt_poses = {}'.format(gt_poses.shape))\n",
    "print('pred_poses = {}'.format(pred_poses.shape))\n",
    "t_loss = np.asarray([np.linalg.norm(p - t) for p, t in zip(pred_poses[:, :3], gt_poses[:, :3])])\n",
    "q_loss = np.asarray([quaternion_angular_error(p, t) for p, t in zip(pred_poses[:, 3:], gt_poses[:, 3:])])\n",
    "\n",
    "print('poses_std = {:.3f}'.format(np.linalg.norm(poses_std)))\n",
    "print('Translation(T) error in meters and Rotation(R) error in degrees:')\n",
    "print('T: median = {:.3f}, mean = {:.3f}'.format(np.median(t_loss), np.mean(t_loss)))\n",
    "print('R: median = {:.3f}, mean = {:.3f}'.format(np.median(q_loss), np.mean(q_loss)))\n",
    "\n",
    "# Save for later visualization\n",
    "pred_poses_val = pred_poses\n",
    "gt_poses_val = gt_poses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Prediction and Ground Truth Poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw ground truth in `blue` and predictions in `red` colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_3d_axes_limits(ax, poses, pose_format='quat'):\n",
    "    p_min, p_max, p_mean, p_std = calc_poses_params(poses, pose_format=pose_format)\n",
    "    ax.set_xlim(p_min[0], p_max[0])\n",
    "    ax.set_ylim(p_min[1], p_max[1])\n",
    "    ax.set_zlim(int(p_min[2] - 1), p_max[2])\n",
    "    return p_min, p_max, p_mean, p_std\n",
    "\n",
    "def draw_pred_gt_poses(pred_poses, gt_poses):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    ax.set_xlabel('$X$')\n",
    "    ax.set_ylabel('$Y$')\n",
    "    ax.set_zlabel('$Z$')\n",
    "    ax.view_init(50, 30)\n",
    "\n",
    "    all_poses = np.concatenate((pred_poses, gt_poses))\n",
    "    p_min, _, _, _ = set_3d_axes_limits(ax, all_poses, pose_format='quat')\n",
    "    \n",
    "    draw_poses(ax, pred_poses[:, :3], proj=False, proj_z=int(p_min[2] - 1), c='r', s=60)\n",
    "    draw_poses(ax, gt_poses[:, :3], proj=False, proj_z=int(p_min[2] - 1), c='b', s=60)\n",
    "    for i in range(pred_poses.shape[0]):\n",
    "        pp = pred_poses[i, :3]\n",
    "        gp = gt_poses[i, :3]\n",
    "        pps = np.vstack((pp, gp))\n",
    "        ax.plot(pps[:, 0], pps[:, 1], pps[:, 2], c=(0.7, 0.7, 0.7, 0.4))\n",
    "        \n",
    "    plt.draw()\n",
    "\n",
    "    \n",
    "# print(pred_poses_train)\n",
    "# print(gt_poses_train)\n",
    "\n",
    "# Draw predicted vs ground truth poses\n",
    "draw_pred_gt_poses(pred_poses_train, gt_poses_train)\n",
    "plt.title('PoseNet on Train Dataset')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "draw_pred_gt_poses(pred_poses_val, gt_poses_val)\n",
    "plt.title('PoseNet on Validation Dataset')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "# n_epochs = 4203\n",
    "save_checkpoint(model, optimizer, 'nb_{}'.format(experiment_name), n_epochs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
